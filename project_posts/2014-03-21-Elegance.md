so, it would be REALLY cool to store the #twitter feed and semantically analyze it.  That way I can 
connect images that have related tags, and the never-ending comic book feed will be all the more cogent.  It's one thing 
to produce meaning by juxtaposition, it's a deeper experience when that juxtaposition is driven by
a taxonomy.  Can it happen before March 28th?  hard to say...

my coding TODos/challenges in the coming week/end - it's a lot but it'll be fun!!!

1)load/retrieve images into/from couchdb through node.js.  ofx-OSC-node-osc 
shouldn't be too difficult.
Since I'm going with images vs video in this first iteration, I'm not worried about needing any 
kind of high octane storage facility.  Couchdb is fast/lean and I'll set and retrieve the images 
from openFrameworks/C++ to local storage.

2)UI - need to alternatingly show images, fade in/fade out. I don't love oF for this.  alphaBlending
is way more complicated than it needs to be (unless maybe I look into ofxTween) and I certainly
don't need any shaders at this point.

3) decide how I'm going to hook up the Twitter API and Google Speech.  I think that in a museum 
environment, processing audio may be a challenge, hence it's my intention to also offer a keyboard implementation.  Also,
in terms of twitter, do I want to make them use their own twitter handle, which involves the log in
and all that madness?  Isn't it just easier to create a twitter account for this project and take
advantage of the hashtags and @s to attach the participant's handle if desired?  

4) it's out of scope for March 28th, but I still want to present an "audience" as a counter-balance to the comic-book-feed eg, Google 'cartoon/black-and-white' Images that match the participant's gesture and even facial expression...
